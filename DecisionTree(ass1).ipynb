{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14bdf392-eb02-4b6a-a006-f7b1b1dbd986",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n",
    "\n",
    "## It is a machine learning algorithm that is a tree like model for making a prediction\n",
    "## starting from root node which represent the entire dataset \n",
    "##Feature selection: At each node, the algorithm selects the feature that best splits the data into subsets.\n",
    "## Creating branches: Based on the selected feature, the dataset is split into branches, leading to new nodes.\n",
    "## Making predictions: To make a prediction, a new instance is passed down the tree, following the decisions at each node until it reaches a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe532ce-3a25-4967-8c41-3fdb423fbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n",
    "\n",
    "##Entropy: This measures the impurity or uncertainty in a group of examples. If all examples are of the same class, entropy is 0 (no uncertainty).\n",
    "\n",
    "## Entropy(S)=-p(+)logp(+)-p(-)logp(-)\n",
    "## Gini Impurity: An alternative to entropy, it measures the frequency at which any element of the dataset will be\n",
    "##  mislabeled if it was randomly labeled according to the distribution of labels in the dataset\n",
    "## distribution of labels in the dataset. It’s calculated as:\n",
    "##Gini Impurity(S)=1−i=1to n ∑pi2*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8dc3e6b-e2cb-4c0f-a3a7-7dd5e4efde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n",
    "\n",
    "## Certainly! A decision tree classifier is particularly well-suited for binary classification problems\n",
    "## Data Preparation: Collect a dataset with features and binary labels (e.g., True or False, 0 or 1).\n",
    "## Tree Construction:\n",
    "## Begin at the root node with the entire dataset.\n",
    "## Choose the best feature to split the data based on a metric like information gain or Gini impurity.\n",
    "## Pruning (Optional):\n",
    "## To avoid overfitting, prune the tree by removing branches that contribute little to prediction accuracy.\n",
    "## Pruning can be done by removing nodes if they don’t significantly increase purity or decrease error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15da63d-2800-476c-bd30-31d2153aa73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "## classification model.\n",
    "\n",
    "## A confusion matrix is a table used to evaluate the performance of a classification model. \n",
    "## The matrix is a 2x2 table with the actual classes on one axis (usually the vertical axis) and the predicted classes on the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "408eb85b-d077-45ee-87e3-0a411713cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "##calculated from it.\n",
    "\n",
    "##precision:the portion of positive identification(TP/TP+fP)\n",
    "##THE PORTION OF ACTUAL positive is the recall (tp/tp+Fn)\n",
    "##he harmonic mean of precision and recall.\n",
    "##F1 Score=2×Precision+RecallPrecision×Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccaadb1b-a437-4b0f-b350-960d0e8fa020",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "##explain why.\n",
    "\n",
    "## An example of a classification problem where precision is the most important metric is in email spam detection. In this scenario,\n",
    "##the goal is to classify emails as either “spam” or “not spam.”\n",
    "## False Positives Impact: If an email is incorrectly classified as spam (false positive), a user might miss important emails\n",
    "## Cost of Errors: The cost of missing a legitimate email is usually higher than the cost of occasionally having to deal with a spam email in the inbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d24634b-01d7-41a0-83d4-6626d60eefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
    "##why.\n",
    "\n",
    "##An example of a classification problem where recall is the most important metric is in medical diagnostics, specifically for detecting \n",
    "##  detecting serious diseases such as cancer.\n",
    "##False Negatives Impact: If a patient with cancer is incorrectly classified as not having cancer (false negative), \n",
    "##  the consequences can be life-threatening. Early detection is crucial for treatment and survival chances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408f2a2-9dc8-4fa0-bf65-d3a8d012339a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc261f3b-c5df-4c71-b715-ae2df7bf0b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

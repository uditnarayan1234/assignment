{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662e3469-07b8-4a23-b112-ecec0a99ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q1. What is boosting in machine learning?\n",
    "\n",
    "## Boosting is an ensemble modeling technique used in machine learning to improve the predictive accuracy of models by combining multiple weak learners.\n",
    "## Boosting builds a sequence of models (usually decision trees) in an iterative manner.\n",
    "## Each model corrects the errors made by the previous one.\n",
    "## The final prediction is an aggregation of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8a6700-2ed3-4b9d-87e8-5f1e2879cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q2. What are the advantages and limitations of using boosting techniques?\n",
    "\n",
    "## Improved Accuracy: Boosting can significantly improve the accuracy of weak models by combining their predictions. \n",
    "## Handling Imbalanced Data: Boosting handles imbalanced datasets well by assigning higher weights to misclassified instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33415286-a518-4d68-b3a7-54009f20dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q3. Explain how boosting works.\n",
    "\n",
    "## Sequential Learning: Boosting builds a sequence of models (often decision trees) iteratively.\n",
    "## Weighted Training: Each model corrects the errors made by its predecessor.\n",
    "## Weighted Aggregation: The final prediction is an aggregation of all these models.\n",
    "## Data Re-weighting: After adding a weak learner, data weights are adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e0cbb36-9dca-4ce6-90de-36a9c475d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q4. What are the different types of boosting algorithms?\n",
    "\n",
    "## 3 types of boosting algorithm 1. adaboost 2. xgboost 3. gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fac6dc-e228-4c15-b856-10e1d89c8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q5. What are some common parameters in boosting algorithms?\n",
    "\n",
    "## n_estimators (Number of Weak Learners):\n",
    "## This parameter determines the maximum number of weak learners (base models) that the boosting algorithm is allowed to build.\n",
    "\n",
    "## base_estimator (Weak Learner):\n",
    "## This parameter specifies the algorithm used as the base model (weak learner).\n",
    "\n",
    "## max_depth (Tree Depth):\n",
    "## If the base estimator is a decision tree, you can control its depth using max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba50ca0-eaeb-4199-aac8-d3c97514697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q6. How do boosting algorithms combine weak learners to create a strong learner?\n",
    "\n",
    "## In ensemble learning, we often describe models as either weak learners or strong learners.\n",
    "## A weak learner is a model that performs slightly better than random guessing. It produces predictions with some skill but not significantly better than chance.\n",
    "## A strong learner, on the other hand, achieves arbitrarily good accuracy and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95a124c9-ce91-47e8-89ba-ab36d76907bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q7. Explain the concept of AdaBoost algorithm and its working.\n",
    "\n",
    "## Here’s how AdaBoost works step by step: a. Initialization:\n",
    "## Assign equal weights to all training examples (initially, each example has the same importance).\n",
    "## Select a weak learner (usually a decision tree with only one level, also known as a stump).\n",
    "## b. Iterative Training:\n",
    "## Train the weak learner on the weighted training data.\n",
    "## Evaluate its performance (accuracy) on the entire dataset.\n",
    "## Adjust the weights of misclassified examples:\n",
    "## Increase the weights of misclassified examples (emphasizing their importance).\n",
    "## Decrease the weights of correctly classified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8cb4be6-916d-4402-b4d2-c7d6fa4aa286",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q8. What is the loss function used in AdaBoost algorithm?\n",
    "\n",
    "## In the AdaBoost (Adaptive Boosting) algorithm, the loss function used for optimization is the exponential loss\n",
    "# Exponential Loss:\n",
    "## Exponential Loss:\n",
    "# The exponential loss function is defined as: [ L(y, f(x)) = e^{-yf(x)} ]\n",
    "## (y) represents the true class label (either +1 or -1).\n",
    "## (f(x)) represents the output of the weak classifier (e.g., decision stump)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a70a70b7-ed7c-42f0-8a0d-db5f2956b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?\n",
    "\n",
    "## Increasing the number of estimators (also known as weak learners or base models) in the AdaBoost algorithm has both advantages and limitations\n",
    "## Overfitting Risk: While increasing the number of estimators initially improves performance, there’s a point beyond which the model may start overfitting the training data.\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add1bed-690f-4349-8f6b-ccded631f71f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
